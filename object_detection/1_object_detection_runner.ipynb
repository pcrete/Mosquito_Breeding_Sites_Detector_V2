{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breeding Site Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import time\n",
    "import collections\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.insert(0,os.path.abspath(\"..\"))\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing GSV images corresponding to subdistricts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(FILES, DIR, province, district, subdist): \n",
    "    with detection_graph.as_default():\n",
    "        with tf.Session(graph=detection_graph) as sess:\n",
    "\n",
    "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "            detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "            detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "            detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "            \n",
    "            for image_path in tqdm(FILES):\n",
    "                image = Image.open(os.path.join(DIR, 'original', image_path))\n",
    "                image_np = load_image_into_numpy_array(image)\n",
    "                \n",
    "                image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "                (boxes, scores, classes, num) = sess.run(\n",
    "                    [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "                    feed_dict={image_tensor: image_np_expanded}\n",
    "                )\n",
    "\n",
    "                indexes, cls_count = count(np.squeeze(boxes),\n",
    "                                           np.squeeze(classes).astype(np.int32),\n",
    "                                           np.squeeze(scores),\n",
    "                                           breading_index)\n",
    "                \n",
    "                export_json(DIR, image_path, \n",
    "                            np.squeeze(boxes), \n",
    "                            np.squeeze(classes).astype(np.int32),\n",
    "                            np.squeeze(scores),\n",
    "                            breading_index,\n",
    "                            indexes, cls_count,\n",
    "                            province, district, subdist\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of detected breeding sites for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(boxes, classes, scores, breading_index, max_boxes_to_draw=30):\n",
    "    cls_count = {}\n",
    "    i, count = 0, 0\n",
    "    indexes = []\n",
    "    while(i < boxes.shape[0] and count < max_boxes_to_draw):\n",
    "        if classes[i] in breading_index.keys():\n",
    "            if scores[i] > breading_index[classes[i]]['threshold']:\n",
    "                class_name = breading_index[classes[i]]['name']\n",
    "                \n",
    "                if class_name == 'car': \n",
    "                    i+=1\n",
    "                    continue\n",
    "                    \n",
    "                if class_name not in cls_count:\n",
    "                    cls_count[class_name] = 0\n",
    "                cls_count[class_name] += 1\n",
    "                \n",
    "                indexes.append(i)\n",
    "                count+=1\n",
    "        i+=1\n",
    "    return indexes, cls_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the results to Json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_json(DIR ,image_path, boxes, classes, scores, breading_index, indexes, cls_count, \n",
    "                province, district, subdist):\n",
    "    \n",
    "    lat, lng, angle, date = image_path[:-4].split('_')\n",
    "    data =  {\n",
    "        \"province\": province,\n",
    "        \"district\": district,\n",
    "        \"subdist\": subdist,\n",
    "        \"image_name\": image_path,\n",
    "        \"coordinate\": [float(lat), float(lng)],\n",
    "        \"angle\": int(angle),\n",
    "        \"date\":{\n",
    "            \"month\": date.split('-')[1],\n",
    "            \"year\": date.split('-')[0]\n",
    "        },\n",
    "        \"breading_boxes\": indexes,\n",
    "        \"count\": cls_count,\n",
    "        \"boxes\": []\n",
    "    }\n",
    "    for i in range(len(classes)):\n",
    "        if i == 30: break\n",
    "        data[\"boxes\"].append({\n",
    "                            \"bndbox\": boxes[i].tolist(),\n",
    "                            \"score\": scores[i].item(),\n",
    "                            \"cls_name\": breading_index[classes[i]]['name'],\n",
    "                            \"cls_id\": classes[i].item()\n",
    "                        })\n",
    "        \n",
    "\n",
    "    file_name = os.path.join(DIR, 'json', image_path[:-4]+'.json')\n",
    "    \n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define detection thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "breading_index ={1: {'id': 1, 'name': 'bin',         'threshold':0.5},\n",
    "                 2: {'id': 2, 'name': 'bowl',        'threshold':0.5},\n",
    "                 3: {'id': 3, 'name': 'bucket',      'threshold':0.5},\n",
    "                 4: {'id': 4, 'name': 'car',         'threshold':0.5},\n",
    "                 5: {'id': 5, 'name': 'cup',         'threshold':0.5},\n",
    "                 6: {'id': 6, 'name': 'jar',         'threshold':0.5},\n",
    "                 7: {'id': 7, 'name': 'pottedplant', 'threshold':0.5},\n",
    "                 8: {'id': 8, 'name': 'tire',        'threshold':0.5},\n",
    "                 9: {'id': 9, 'name': 'vase',        'threshold':0.5}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'dengue_inference_graph'\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph_v3.pb'\n",
    "PATH_TO_LABELS = os.path.join('dengue_inference_graph', 'dengue_label_map.pbtxt')\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the list of directories or subdistricts\n",
    "That contains path_to_directory, province, district, and subdistrict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/GSV', 'กรุงเทพมหานคร', 'เขตดุสิต', 'แขวงสวนจิตรลดา']\n"
     ]
    }
   ],
   "source": [
    "directories = []\n",
    "\n",
    "province = 'กรุงเทพมหานคร'\n",
    "DIR = '../data/GSV'\n",
    "\n",
    "districts = os.listdir(os.path.join(DIR, province))\n",
    "for district in districts:\n",
    "    subdists = os.listdir(os.path.join(DIR, province, district))\n",
    "    for subdist in subdists:\n",
    "        directories.append([DIR, province, district, subdist])\n",
    "\n",
    "print(directories[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating though directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0 :\tDirectory: ../data/GSV/กรุงเทพมหานคร/เขตดุสิต/แขวงสวนจิตรลดา \tTotal: 1 Files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.65s/it]\n"
     ]
    }
   ],
   "source": [
    "last = 0\n",
    "for k, directory in enumerate(directories[last:]):\n",
    "    DIR, province, district, subdist = directory\n",
    "    directory = os.path.join(DIR, province, district, subdist)\n",
    "    \n",
    "    image_path = os.path.join(directory, 'original')\n",
    "\n",
    "    if not os.path.exists(image_path): continue\n",
    "\n",
    "    if not os.path.exists(os.path.join(directory, 'json')):\n",
    "        os.makedirs(os.path.join(directory, 'json'))\n",
    "\n",
    "    FILES = os.listdir(image_path)\n",
    "    FILES.sort()\n",
    "\n",
    "    print('Index:',k+last,':\\tDirectory:',directory, '\\tTotal:',len(FILES),'Files')\n",
    "\n",
    "    process(FILES, directory, province, district, subdist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
